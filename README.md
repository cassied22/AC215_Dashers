## Milestone 2

<!-- ```
The files are empty placeholders only. You may adjust this template as appropriate for your project.
Never commit large data files,trained models, personal API Keys/secrets to GitHub
``` -->

#### Project Milestone 2 Organization

```
â”œâ”€â”€ .dvc
â”‚   â”œâ”€â”€ .gitignore
â”‚   â”œâ”€â”€ cache
â”‚   â”‚   â””â”€â”€ files
â”‚   â”œâ”€â”€ config
â”‚   â”œâ”€â”€ config.local
â”‚   â””â”€â”€ tmp
â”‚       â”œâ”€â”€ btime
â”‚       â”œâ”€â”€ lock
â”‚       â”œâ”€â”€ rwlock
â”‚       â”œâ”€â”€ rwlock.lock
â”‚       â”œâ”€â”€ updater
â”‚       â””â”€â”€ updater.lock
â”œâ”€â”€ .dvcignore
â”œâ”€â”€ .git
â”‚   â”œâ”€â”€ COMMIT_EDITMSG
â”‚   â”œâ”€â”€ FETCH_HEAD
â”‚   â”œâ”€â”€ HEAD
â”‚   â”œâ”€â”€ ORIG_HEAD
â”‚   â”œâ”€â”€ branches
â”‚   â”œâ”€â”€ config
â”‚   â”œâ”€â”€ description
â”‚   â”œâ”€â”€ hooks
â”‚   â”œâ”€â”€ index
â”‚   â”œâ”€â”€ info
â”‚   â”‚   â””â”€â”€ exclude
â”‚   â”œâ”€â”€ logs
â”‚   â”‚   â”œâ”€â”€ HEAD
â”‚   â”‚   â””â”€â”€ refs
â”‚   â”œâ”€â”€ objects
â”‚   â”œâ”€â”€ packed-refs
â”‚   â””â”€â”€ refs
â”‚       â”œâ”€â”€ heads
â”‚       â”œâ”€â”€ remotes
â”‚       â””â”€â”€ tags
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ recipe_qa.csv
â”‚   â””â”€â”€ recipe_qa.csv.dvc
â”œâ”€â”€ notebooks
â”‚   â”œâ”€â”€ Object_detection_documentation.md
â”‚   â”œâ”€â”€ container.ipynb
â”‚   â”œâ”€â”€ dvc_retrieval.ipynb
â”‚   â”œâ”€â”€ eda.ipynb
â”‚   â”œâ”€â”€ fig_container
â”‚   â”‚   â””â”€â”€ rag_container.png
â”‚   â”œâ”€â”€ fig_llm_performance
â”‚   â”‚   â”œâ”€â”€ raw-rag3-1.png
â”‚   â”‚   â”œâ”€â”€ raw_rag1-1.png
â”‚   â”‚   â”œâ”€â”€ raw_rag1-2.png
â”‚   â”‚   â”œâ”€â”€ raw_rag1-3.png
â”‚   â”‚   â”œâ”€â”€ raw_rag2-1.png
â”‚   â”‚   â”œâ”€â”€ raw_rag2-2.png
â”‚   â”‚   â””â”€â”€ raw_rag3-2.png
â”‚   â”œâ”€â”€ food
â”‚   â”‚   â”œâ”€â”€ food1.jpg
â”‚   â”‚   â”œâ”€â”€ food1_gemini.png
â”‚   â”‚   â”œâ”€â”€ food1_gpt.png
â”‚   â”‚   â”œâ”€â”€ food1_mediapipe.png
â”‚   â”‚   â”œâ”€â”€ food1_yolov8.png
â”‚   â”‚   â”œâ”€â”€ food2.png
â”‚   â”‚   â”œâ”€â”€ food2_gemini.png
â”‚   â”‚   â”œâ”€â”€ food2_gpt.png
â”‚   â”‚   â”œâ”€â”€ food2_mediapipe.png
â”‚   â”‚   â”œâ”€â”€ food2_yolov8.png
â”‚   â”‚   â”œâ”€â”€ food3.jpg
â”‚   â”‚   â”œâ”€â”€ food3_gemini.png
â”‚   â”‚   â”œâ”€â”€ food3_gpt.png
â”‚   â”‚   â”œâ”€â”€ food3_mediapipe.png
â”‚   â”‚   â””â”€â”€ food3_yolov8.png
â”‚   â””â”€â”€ llm_performance.ipynb
â”œâ”€â”€ references
â”‚   â””â”€â”€ .gitkeep
â”œâ”€â”€ reports
â”‚   â”œâ”€â”€ Daily Meal Assistant Prototype.pdf
â”‚   â”œâ”€â”€ Prototype_v2.pdf
â”‚   â””â”€â”€ Statement of Work_Sample.pdf
â””â”€â”€ src
    â”œâ”€â”€ data-versioning
    â”‚   â”œâ”€â”€ .gitignore
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ Pipfile
    â”‚   â”œâ”€â”€ Pipfile.lock
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ docker-entrypoint.sh
    â”‚   â””â”€â”€ docker-shell.sh
    â”œâ”€â”€ datapipeline
    â”‚   â”œâ”€â”€ .gitignore
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ Pipfile
    â”‚   â”œâ”€â”€ Pipfile.lock
    â”‚   â”œâ”€â”€ cli_rag.py
    â”‚   â”œâ”€â”€ dataloader.py
    â”‚   â”œâ”€â”€ docker-compose.yml
    â”‚   â”œâ”€â”€ docker-entrypoint.sh
    â”‚   â”œâ”€â”€ docker-shell.sh
    â”‚   â”œâ”€â”€ input-datasets
    â”‚   â”œâ”€â”€ outputs
    â”‚   â”œâ”€â”€ preprocess_cv.py
    â”‚   â””â”€â”€ requirements.txt
    â”œâ”€â”€ food-detection
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ Pipfile
    â”‚   â”œâ”€â”€ data
    â”‚   â”œâ”€â”€ docker-compose.yml
    â”‚   â”œâ”€â”€ docker-shell.sh
    â”‚   â”œâ”€â”€ gemini-object-detection.py
    â”‚   â”œâ”€â”€ gpt-object-detection.py
    â”‚   â””â”€â”€ requirements.txt
    â”œâ”€â”€ llm_finetuning
    â”‚   â”œâ”€â”€ .gitignore
    â”‚   â”œâ”€â”€ README.md
    â”‚   â”œâ”€â”€ dataset-creator
    â”‚   â”œâ”€â”€ env.dev
    â”‚   â”œâ”€â”€ gemini-finetuner
    â”‚   â””â”€â”€ images
    â”œâ”€â”€ models
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â”œâ”€â”€ docker-shell.sh
    â”‚   â”œâ”€â”€ infer_model.py
    â”‚   â”œâ”€â”€ model_rag.py
    â”‚   â””â”€â”€ train_model.py
    â””â”€â”€ secrets
        â””â”€â”€ .gitkeep
```

# AC215 - Milestone2 - Daily Meal Assistant - "What to Eat Today"

**Team Members**

Hanqi(Hanna) Zeng(hanqizeng@hsph.harvard.edu)  <br/> 
Chris Wang(ywang3@hsph.harvard.edu)   <br/> 
Selina Qian(jingyun_qian@hsph.harvard.edu) <br/> 
Shiyu Ma(shiyuma@g.harvard.edu)  <br/> 
Cassie Dai(cdai@g.harvard.edu) <br/> 


**Group Name**
Dashers

**Project**
In this project, we aim to develop an app that serves as a personal meal assistant, helping users track their available ingredients, suggest healthy recipes, and recommend nearby restaurants based on user preferences and current inventory. The app will combine advanced AI tools like object detection and large language models (LLMs) to provide tailored meal recommendations and route suggestions for dining out. <br/>

The app pipeline flow is as shown [here](https://github.com/cassied22/AC215_Dashers/blob/milestone2/reports/Prototype_v2.pdf).

### Milestone2 ###

In this milestone, we have the components for data management, including versioning, as well as the computer vision and language models.

**Data**
We gathered a dataset of 2,231,150 recipes including title (dish name), ingredients, directions, retrieved link, source type, and named entity recognition (NER) for food items. The ingredients are listed as an array of strings. The directions are provided as an array of strings, with each string representing a step in the cooking process. The NER data consists of an array of food item names extracted from the recipe. The dataset, approximately 2.29 GB in size, was collected from the following source: https://huggingface.co/datasets/mbien/recipe_nlg. We have stored it in a private Google Cloud Bucket.

**Data Pipeline Containers**

1. One container prepares data for the RAG model, including tasks such as chunking, embedding, and populating the vector database.

## Data Pipeline Overview

1. **`src/datapipeline/cli_rag.py`**
   This script handles preprocessing on our 2.29 GB dataset in the `input-datasets` folder. It does data cleaning and feature selection to enable faster iteration during processing. The preprocessed dataset is now reduced to 676 MB and stored on GCS. The `outputs` folder is a temperary path where the local copy of the vector database is stored. 
   This script also prepares the necessary data for setting up our vector database. It performs chunking, embedding, and loads the data into a vector database (ChromaDB).

2. **`src/datapipeline/Pipfile`**
   We used the following packages to help with preprocessing:
   - `user-agent requests google-cloud-storage google-generativeai google-cloud-aiplatform pandas langchain llama-index chromadb langchain-community pyarrow`

3. **`src/preprocessing/Dockerfile(s)`**
   Our Dockerfiles follow standard conventions, with the exception of some specific modifications described in the Dockerfile/described below.


## Running Dockerfile
Go to a terminal inside datapipeline
- Run docker container by using:
```sh docker-shell.sh```

## Mock Submission

<br/>
Login GCP, select project id, our ```x-goog-project-id```, start the VM instance.<br/>
Open a GCP terminal, change directory into corresponding folder with Dockerfile. <br/>
Run docker-shell.sh using command: ```sudo sh docker-shell.sh``` <br/>
Inside the container, run preprocessing using command: ```python cli_rag.py```. <br/>
cli_rag.py would run the RAG LLM. You could observe the updates in Cloud Storage - Buckets in your GCP project. <br/>

<!-- **Notebooks/Reports**
This folder contains code that is not part of container - for e.g: Application mockup, EDA, any ðŸ” ðŸ•µï¸â€â™€ï¸ ðŸ•µï¸â€â™‚ï¸ crucial insights, reports or visualizations. -->

