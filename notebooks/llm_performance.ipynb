{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Gemini model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model performs generative tasks, we currently lack quantitative metrics to evaluate the model. However, we plan to conduct human quanlitative evaluation of our model's performance. Below are the experiment logs of our model's generated texts. We will systematically evaluate these experiment results after collecting more sample responses. For example, we can give scores to evaluate on recipe's relatedness to user's need (calory, healthyness, styles etc.), text formatting and grammar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test case 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](fig_llm_performance/raw_rag1-1.png)\n",
    "![](fig_llm_performance/raw_rag1-2.png)\n",
    "![](fig_llm_performance/raw_rag1-3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test case 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](fig_llm_performance/raw_rag2-1.png)\n",
    "![](fig_llm_performance/raw_rag2-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test case 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](fig_llm_performance/raw_rag3-1.png)\n",
    "![](fig_llm_performance/raw_rag3-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
